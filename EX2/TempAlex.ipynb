{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_test_table\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_test_table()\n",
    "X_train = X_train.drop([\"Unnamed: 0\"], axis=1)\n",
    "X_test=X_test.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X_train' and 'X_test' are pandas DataFrames and have a date column named 'date'\n",
    "X_train['date'] = pd.to_datetime(X_train['date'])\n",
    "X_test['date'] = pd.to_datetime(X_test['date'])\n",
    "\n",
    "# Sort data by date\n",
    "X_train = X_train.sort_values(by='date')\n",
    "X_test = X_test.sort_values(by='date')\n",
    "\n",
    "# Assign a unique number to each date\n",
    "unique_dates = pd.concat([X_train['date'], X_test['date']]).unique()\n",
    "date_to_number = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "X_train['date'] = X_train['date'].map(date_to_number).astype('float32')\n",
    "X_test['date'] = X_test['date'].map(date_to_number).astype('float32')\n",
    "\n",
    "# Convert the rest of your dataframe to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" from keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\nfrom keras import backend as K\\n\\ndef rmse(y_true, y_pred):\\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\\n\\nmodel = Sequential()\\nmodel.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\\nmodel.add(Dense(1))\\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmse])\\n\\nmodel.fit(X_train, y_train, epochs=5, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False) \""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmse])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1336/1336 [==============================] - 10s 5ms/step - loss: 63.9408 - rmse: 7.1704 - val_loss: 60.0468 - val_rmse: 7.0312\n",
      "Epoch 2/3\n",
      "1336/1336 [==============================] - 6s 5ms/step - loss: 62.6471 - rmse: 7.1182 - val_loss: 60.0635 - val_rmse: 7.0359\n",
      "Epoch 3/3\n",
      "1336/1336 [==============================] - 6s 5ms/step - loss: 62.6138 - rmse: 7.1187 - val_loss: 60.0527 - val_rmse: 7.0329\n",
      "Epoch 1/5\n",
      "1336/1336 [==============================] - 10s 5ms/step - loss: 63.7631 - rmse: 7.1592 - val_loss: 60.0630 - val_rmse: 7.0357\n",
      "Epoch 2/5\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.6558 - rmse: 7.1214 - val_loss: 60.0657 - val_rmse: 7.0365\n",
      "Epoch 3/5\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5368 - rmse: 7.1130 - val_loss: 60.0608 - val_rmse: 7.0351\n",
      "Epoch 4/5\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5777 - rmse: 7.1142 - val_loss: 60.0532 - val_rmse: 7.0330\n",
      "Epoch 5/5\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5776 - rmse: 7.1149 - val_loss: 60.0438 - val_rmse: 7.0302\n",
      "Epoch 1/7\n",
      "1336/1336 [==============================] - 10s 5ms/step - loss: 63.7638 - rmse: 7.1657 - val_loss: 60.0598 - val_rmse: 7.0349\n",
      "Epoch 2/7\n",
      "1336/1336 [==============================] - 6s 5ms/step - loss: 62.6981 - rmse: 7.1210 - val_loss: 60.0387 - val_rmse: 7.0287\n",
      "Epoch 3/7\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5432 - rmse: 7.1124 - val_loss: 60.0596 - val_rmse: 7.0348\n",
      "Epoch 4/7\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.6128 - rmse: 7.1174 - val_loss: 60.0604 - val_rmse: 7.0350\n",
      "Epoch 5/7\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5280 - rmse: 7.1115 - val_loss: 60.0340 - val_rmse: 7.0273\n",
      "Epoch 6/7\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.4968 - rmse: 7.1113 - val_loss: 60.0456 - val_rmse: 7.0308\n",
      "Epoch 7/7\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5448 - rmse: 7.1129 - val_loss: 60.0448 - val_rmse: 7.0306\n",
      "Epoch 1/10\n",
      "1336/1336 [==============================] - 10s 5ms/step - loss: 64.7703 - rmse: 7.2024 - val_loss: 60.0475 - val_rmse: 7.0313\n",
      "Epoch 2/10\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.9001 - rmse: 7.1308 - val_loss: 59.9994 - val_rmse: 7.0156\n",
      "Epoch 3/10\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.7112 - rmse: 7.1198 - val_loss: 60.0157 - val_rmse: 7.0214\n",
      "Epoch 4/10\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.5950 - rmse: 7.1138 - val_loss: 60.0481 - val_rmse: 7.0315\n",
      "Epoch 5/10\n",
      "1336/1336 [==============================] - 5s 4ms/step - loss: 62.5231 - rmse: 7.1104 - val_loss: 60.0692 - val_rmse: 7.0374\n",
      "Epoch 6/10\n",
      "1336/1336 [==============================] - 5s 4ms/step - loss: 62.5380 - rmse: 7.1111 - val_loss: 60.0503 - val_rmse: 7.0322\n",
      "Epoch 7/10\n",
      "1336/1336 [==============================] - 5s 4ms/step - loss: 62.4903 - rmse: 7.1104 - val_loss: 60.0414 - val_rmse: 7.0296\n",
      "Epoch 8/10\n",
      "1336/1336 [==============================] - 5s 4ms/step - loss: 62.4832 - rmse: 7.1102 - val_loss: 60.0521 - val_rmse: 7.0327\n",
      "Epoch 9/10\n",
      "1336/1336 [==============================] - 5s 4ms/step - loss: 62.4851 - rmse: 7.1087 - val_loss: 60.0237 - val_rmse: 7.0241\n",
      "Epoch 10/10\n",
      "1336/1336 [==============================] - 6s 4ms/step - loss: 62.4639 - rmse: 7.1091 - val_loss: 60.0744 - val_rmse: 7.0388\n",
      "Epoch 1/3\n",
      "668/668 [==============================] - 7s 5ms/step - loss: 64.8792 - rmse: 7.5129 - val_loss: 59.9850 - val_rmse: 7.2690\n",
      "Epoch 2/3\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.8157 - rmse: 7.4122 - val_loss: 60.0318 - val_rmse: 7.2764\n",
      "Epoch 3/3\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.6606 - rmse: 7.4066 - val_loss: 60.0403 - val_rmse: 7.2782\n",
      "Epoch 1/5\n",
      "668/668 [==============================] - 7s 5ms/step - loss: 64.8935 - rmse: 7.5137 - val_loss: 60.0327 - val_rmse: 7.2766\n",
      "Epoch 2/5\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.6942 - rmse: 7.4052 - val_loss: 60.0342 - val_rmse: 7.2769\n",
      "Epoch 3/5\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5570 - rmse: 7.3973 - val_loss: 60.0482 - val_rmse: 7.2798\n",
      "Epoch 4/5\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4922 - rmse: 7.3944 - val_loss: 60.0433 - val_rmse: 7.2788\n",
      "Epoch 5/5\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5501 - rmse: 7.3980 - val_loss: 60.0482 - val_rmse: 7.2798\n",
      "Epoch 1/7\n",
      "668/668 [==============================] - 6s 5ms/step - loss: 64.8732 - rmse: 7.5104 - val_loss: 60.0421 - val_rmse: 7.2786\n",
      "Epoch 2/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.8040 - rmse: 7.4116 - val_loss: 60.0187 - val_rmse: 7.2735\n",
      "Epoch 3/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.6116 - rmse: 7.3994 - val_loss: 60.0457 - val_rmse: 7.2793\n",
      "Epoch 4/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5970 - rmse: 7.4012 - val_loss: 60.0409 - val_rmse: 7.2783\n",
      "Epoch 5/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5571 - rmse: 7.4008 - val_loss: 60.0582 - val_rmse: 7.2818\n",
      "Epoch 6/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5802 - rmse: 7.3995 - val_loss: 60.0237 - val_rmse: 7.2746\n",
      "Epoch 7/7\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5425 - rmse: 7.3968 - val_loss: 60.0186 - val_rmse: 7.2735\n",
      "Epoch 1/10\n",
      "668/668 [==============================] - 6s 5ms/step - loss: 64.9126 - rmse: 7.5171 - val_loss: 60.0291 - val_rmse: 7.2758\n",
      "Epoch 2/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.8089 - rmse: 7.4107 - val_loss: 60.0205 - val_rmse: 7.2739\n",
      "Epoch 3/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.6285 - rmse: 7.4015 - val_loss: 60.0541 - val_rmse: 7.2810\n",
      "Epoch 4/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5529 - rmse: 7.3983 - val_loss: 60.0344 - val_rmse: 7.2769\n",
      "Epoch 5/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4268 - rmse: 7.3896 - val_loss: 60.0538 - val_rmse: 7.2809\n",
      "Epoch 6/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4529 - rmse: 7.3926 - val_loss: 60.0492 - val_rmse: 7.2800\n",
      "Epoch 7/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4824 - rmse: 7.3933 - val_loss: 60.0313 - val_rmse: 7.2763\n",
      "Epoch 8/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4804 - rmse: 7.3926 - val_loss: 60.0488 - val_rmse: 7.2800\n",
      "Epoch 9/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.4997 - rmse: 7.3950 - val_loss: 60.0344 - val_rmse: 7.2769\n",
      "Epoch 10/10\n",
      "668/668 [==============================] - 3s 4ms/step - loss: 62.5480 - rmse: 7.3982 - val_loss: 60.0294 - val_rmse: 7.2759\n",
      "Epoch 1/3\n",
      "334/334 [==============================] - 5s 7ms/step - loss: 66.8002 - rmse: 7.8451 - val_loss: 60.0223 - val_rmse: 7.4530\n",
      "Epoch 2/3\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.9018 - rmse: 7.6259 - val_loss: 60.0691 - val_rmse: 7.4598\n",
      "Epoch 3/3\n",
      "334/334 [==============================] - 2s 5ms/step - loss: 62.7670 - rmse: 7.6199 - val_loss: 60.0426 - val_rmse: 7.4560\n",
      "Epoch 1/5\n",
      "334/334 [==============================] - 5s 7ms/step - loss: 66.0114 - rmse: 7.7986 - val_loss: 60.0356 - val_rmse: 7.4561\n",
      "Epoch 2/5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.7585 - rmse: 7.6168 - val_loss: 60.0780 - val_rmse: 7.4610\n",
      "Epoch 3/5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6567 - rmse: 7.6112 - val_loss: 60.0831 - val_rmse: 7.4617\n",
      "Epoch 4/5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6827 - rmse: 7.6142 - val_loss: 60.0764 - val_rmse: 7.4608\n",
      "Epoch 5/5\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6365 - rmse: 7.6104 - val_loss: 60.0748 - val_rmse: 7.4606\n",
      "Epoch 1/7\n",
      "334/334 [==============================] - 5s 7ms/step - loss: 66.8150 - rmse: 7.8495 - val_loss: 60.0558 - val_rmse: 7.4572\n",
      "Epoch 2/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.7777 - rmse: 7.6208 - val_loss: 60.0905 - val_rmse: 7.4627\n",
      "Epoch 3/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6378 - rmse: 7.6100 - val_loss: 60.0973 - val_rmse: 7.4636\n",
      "Epoch 4/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6809 - rmse: 7.6122 - val_loss: 60.0823 - val_rmse: 7.4616\n",
      "Epoch 5/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5206 - rmse: 7.6033 - val_loss: 60.0802 - val_rmse: 7.4613\n",
      "Epoch 6/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6231 - rmse: 7.6109 - val_loss: 60.0782 - val_rmse: 7.4611\n",
      "Epoch 7/7\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5270 - rmse: 7.6046 - val_loss: 60.0850 - val_rmse: 7.4620\n",
      "Epoch 1/10\n",
      "334/334 [==============================] - 7s 7ms/step - loss: 67.1524 - rmse: 7.8665 - val_loss: 60.0556 - val_rmse: 7.4579\n",
      "Epoch 2/10\n",
      "334/334 [==============================] - 2s 5ms/step - loss: 62.7411 - rmse: 7.6190 - val_loss: 60.0816 - val_rmse: 7.4615\n",
      "Epoch 3/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.6985 - rmse: 7.6148 - val_loss: 60.0933 - val_rmse: 7.4631\n",
      "Epoch 4/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.7370 - rmse: 7.6147 - val_loss: 60.0537 - val_rmse: 7.4576\n",
      "Epoch 5/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5655 - rmse: 7.6065 - val_loss: 60.0651 - val_rmse: 7.4593\n",
      "Epoch 6/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5620 - rmse: 7.6043 - val_loss: 60.0623 - val_rmse: 7.4589\n",
      "Epoch 7/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5181 - rmse: 7.6042 - val_loss: 60.0675 - val_rmse: 7.4596\n",
      "Epoch 8/10\n",
      "334/334 [==============================] - 2s 5ms/step - loss: 62.5157 - rmse: 7.6024 - val_loss: 60.0691 - val_rmse: 7.4598\n",
      "Epoch 9/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.4978 - rmse: 7.6024 - val_loss: 60.0672 - val_rmse: 7.4596\n",
      "Epoch 10/10\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 62.5300 - rmse: 7.6040 - val_loss: 60.0664 - val_rmse: 7.4594\n",
      "Epoch 1/3\n",
      "167/167 [==============================] - 5s 9ms/step - loss: 69.4838 - rmse: 8.1434 - val_loss: 60.0618 - val_rmse: 7.6047\n",
      "Epoch 2/3\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.7178 - rmse: 7.7473 - val_loss: 60.0569 - val_rmse: 7.6042\n",
      "Epoch 3/3\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6623 - rmse: 7.7432 - val_loss: 60.0805 - val_rmse: 7.6065\n",
      "Epoch 1/5\n",
      "167/167 [==============================] - 5s 9ms/step - loss: 69.1557 - rmse: 8.1232 - val_loss: 60.0488 - val_rmse: 7.6034\n",
      "Epoch 2/5\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.7908 - rmse: 7.7501 - val_loss: 60.1050 - val_rmse: 7.6088\n",
      "Epoch 3/5\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6448 - rmse: 7.7424 - val_loss: 60.0917 - val_rmse: 7.6075\n",
      "Epoch 4/5\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6430 - rmse: 7.7407 - val_loss: 60.1012 - val_rmse: 7.6084\n",
      "Epoch 5/5\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.5364 - rmse: 7.7368 - val_loss: 60.0836 - val_rmse: 7.6068\n",
      "Epoch 1/7\n",
      "167/167 [==============================] - 5s 9ms/step - loss: 70.8914 - rmse: 8.2253 - val_loss: 60.0282 - val_rmse: 7.6014\n",
      "Epoch 2/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.7891 - rmse: 7.7493 - val_loss: 60.0200 - val_rmse: 7.6005\n",
      "Epoch 3/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.7654 - rmse: 7.7515 - val_loss: 60.0616 - val_rmse: 7.6047\n",
      "Epoch 4/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6953 - rmse: 7.7447 - val_loss: 60.0519 - val_rmse: 7.6037\n",
      "Epoch 5/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.8125 - rmse: 7.7522 - val_loss: 60.0183 - val_rmse: 7.6004\n",
      "Epoch 6/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6827 - rmse: 7.7430 - val_loss: 60.0298 - val_rmse: 7.6015\n",
      "Epoch 7/7\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6233 - rmse: 7.7415 - val_loss: 60.0719 - val_rmse: 7.6057\n",
      "Epoch 1/10\n",
      "167/167 [==============================] - 5s 10ms/step - loss: 70.1053 - rmse: 8.1785 - val_loss: 60.0489 - val_rmse: 7.6034\n",
      "Epoch 2/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.9272 - rmse: 7.7619 - val_loss: 60.0826 - val_rmse: 7.6067\n",
      "Epoch 3/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.8461 - rmse: 7.7540 - val_loss: 60.0576 - val_rmse: 7.6043\n",
      "Epoch 4/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6066 - rmse: 7.7395 - val_loss: 60.0643 - val_rmse: 7.6050\n",
      "Epoch 5/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.5249 - rmse: 7.7338 - val_loss: 60.0685 - val_rmse: 7.6054\n",
      "Epoch 6/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.7161 - rmse: 7.7454 - val_loss: 60.0717 - val_rmse: 7.6057\n",
      "Epoch 7/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.6464 - rmse: 7.7418 - val_loss: 60.0654 - val_rmse: 7.6051\n",
      "Epoch 8/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.4956 - rmse: 7.7328 - val_loss: 60.0808 - val_rmse: 7.6065\n",
      "Epoch 9/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.5322 - rmse: 7.7352 - val_loss: 60.0845 - val_rmse: 7.6069\n",
      "Epoch 10/10\n",
      "167/167 [==============================] - 1s 5ms/step - loss: 62.5301 - rmse: 7.7341 - val_loss: 60.0683 - val_rmse: 7.6053\n",
      "Best score: [60.01854705810547, 7.452374458312988], with parameters: {'batch_size': 16, 'epochs': 7}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [8, 16, 32, 64],\n",
    "    'epochs': [3, 5, 7, 10]\n",
    "}\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_compile_model(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "best_score = None\n",
    "best_params = None\n",
    "\n",
    "for batch_size in param_grid['batch_size']:\n",
    "    for epochs in param_grid['epochs']:\n",
    "\n",
    "            model = create_compile_model('adam')\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, shuffle=False)\n",
    "\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "            if best_score is None or score < best_score:\n",
    "                best_score = score\n",
    "                best_params = {'batch_size': batch_size, 'epochs': epochs}\n",
    "\n",
    "print(f'Best score: {best_score}, with parameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
