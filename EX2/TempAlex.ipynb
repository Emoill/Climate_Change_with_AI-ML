{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_test_table\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_test_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X_train' and 'X_test' are pandas DataFrames and have a date column named 'date'\n",
    "X_train['date'] = pd.to_datetime(X_train['date'])\n",
    "X_test['date'] = pd.to_datetime(X_test['date'])\n",
    "\n",
    "# Sort data by date\n",
    "X_train = X_train.sort_values(by='date')\n",
    "X_test = X_test.sort_values(by='date')\n",
    "\n",
    "# Assign a unique number to each date\n",
    "unique_dates = pd.concat([X_train['date'], X_test['date']]).unique()\n",
    "date_to_number = {date: i for i, date in enumerate(sorted(unique_dates))}\n",
    "X_train['date'] = X_train['date'].map(date_to_number).astype('float32')\n",
    "X_test['date'] = X_test['date'].map(date_to_number).astype('float32')\n",
    "\n",
    "# Convert the rest of your dataframe to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Reshape for LSTM input\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" from keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\nfrom keras import backend as K\\n\\ndef rmse(y_true, y_pred):\\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\\n\\nmodel = Sequential()\\nmodel.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\\nmodel.add(Dense(1))\\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmse])\\n\\nmodel.fit(X_train, y_train, epochs=5, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False) \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmse])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1247/1247 [==============================] - 26s 13ms/step - loss: 64.8629 - rmse: 7.0531 - val_loss: 59.6048 - val_rmse: 6.7604\n",
      "Epoch 2/3\n",
      "1247/1247 [==============================] - 12s 10ms/step - loss: 63.3462 - rmse: 7.0310 - val_loss: 59.4916 - val_rmse: 6.7606\n",
      "Epoch 3/3\n",
      "1247/1247 [==============================] - 15s 12ms/step - loss: 63.2302 - rmse: 7.0219 - val_loss: 59.4611 - val_rmse: 6.7643\n",
      "Epoch 1/5\n",
      "1247/1247 [==============================] - 29s 13ms/step - loss: 64.6607 - rmse: 7.0520 - val_loss: 59.1240 - val_rmse: 6.7955\n",
      "Epoch 2/5\n",
      "1247/1247 [==============================] - 14s 11ms/step - loss: 63.3535 - rmse: 7.0282 - val_loss: 59.1210 - val_rmse: 6.7913\n",
      "Epoch 3/5\n",
      "1247/1247 [==============================] - 14s 11ms/step - loss: 63.2151 - rmse: 7.0231 - val_loss: 59.1210 - val_rmse: 6.7959\n",
      "Epoch 4/5\n",
      "1247/1247 [==============================] - 15s 12ms/step - loss: 63.1592 - rmse: 7.0196 - val_loss: 59.1204 - val_rmse: 6.7924\n",
      "Epoch 5/5\n",
      "1247/1247 [==============================] - 15s 12ms/step - loss: 63.1786 - rmse: 7.0213 - val_loss: 59.1226 - val_rmse: 6.7938\n",
      "Epoch 1/7\n",
      "1247/1247 [==============================] - 24s 12ms/step - loss: 65.1151 - rmse: 7.0548 - val_loss: 59.1093 - val_rmse: 6.7593\n",
      "Epoch 2/7\n",
      "1247/1247 [==============================] - 12s 10ms/step - loss: 63.3513 - rmse: 7.0321 - val_loss: 59.1294 - val_rmse: 6.7495\n",
      "Epoch 3/7\n",
      "1247/1247 [==============================] - 13s 10ms/step - loss: 63.3820 - rmse: 7.0316 - val_loss: 59.1098 - val_rmse: 6.7554\n",
      "Epoch 4/7\n",
      "1247/1247 [==============================] - 15s 12ms/step - loss: 63.2370 - rmse: 7.0229 - val_loss: 59.0870 - val_rmse: 6.7672\n",
      "Epoch 5/7\n",
      "1247/1247 [==============================] - 16s 13ms/step - loss: 63.3017 - rmse: 7.0251 - val_loss: 59.0911 - val_rmse: 6.7679\n",
      "Epoch 6/7\n",
      "1247/1247 [==============================] - 16s 13ms/step - loss: 63.1946 - rmse: 7.0196 - val_loss: 59.0950 - val_rmse: 6.7675\n",
      "Epoch 7/7\n",
      "1247/1247 [==============================] - 12s 10ms/step - loss: 63.2173 - rmse: 7.0193 - val_loss: 59.0969 - val_rmse: 6.7664\n",
      "Epoch 1/10\n",
      "1247/1247 [==============================] - 27s 13ms/step - loss: 64.5853 - rmse: 7.0475 - val_loss: 59.1176 - val_rmse: 6.7953\n",
      "Epoch 2/10\n",
      "1247/1247 [==============================] - 15s 12ms/step - loss: 63.3640 - rmse: 7.0298 - val_loss: 59.1174 - val_rmse: 6.7895\n",
      "Epoch 3/10\n",
      "1247/1247 [==============================] - 13s 10ms/step - loss: 63.3388 - rmse: 7.0288 - val_loss: 59.1174 - val_rmse: 6.7920\n",
      "Epoch 4/10\n",
      "1247/1247 [==============================] - 11s 9ms/step - loss: 63.2087 - rmse: 7.0202 - val_loss: 59.1173 - val_rmse: 6.7892\n",
      "Epoch 5/10\n",
      "1247/1247 [==============================] - 12s 10ms/step - loss: 63.2039 - rmse: 7.0217 - val_loss: 59.1172 - val_rmse: 6.7919\n",
      "Epoch 6/10\n",
      "1247/1247 [==============================] - 11s 9ms/step - loss: 63.2107 - rmse: 7.0218 - val_loss: 59.1172 - val_rmse: 6.7892\n",
      "Epoch 7/10\n",
      "1247/1247 [==============================] - 13s 10ms/step - loss: 63.1264 - rmse: 7.0188 - val_loss: 59.1172 - val_rmse: 6.7927\n",
      "Epoch 8/10\n",
      "1247/1247 [==============================] - 13s 11ms/step - loss: 63.1485 - rmse: 7.0190 - val_loss: 59.1170 - val_rmse: 6.7906\n",
      "Epoch 9/10\n",
      "1247/1247 [==============================] - 12s 9ms/step - loss: 63.1358 - rmse: 7.0177 - val_loss: 59.1171 - val_rmse: 6.7924\n",
      "Epoch 10/10\n",
      "1247/1247 [==============================] - 14s 11ms/step - loss: 63.1418 - rmse: 7.0181 - val_loss: 59.1171 - val_rmse: 6.7932\n",
      "Epoch 1/3\n",
      "624/624 [==============================] - 23s 13ms/step - loss: 65.5036 - rmse: 7.3889 - val_loss: 59.1173 - val_rmse: 7.0818\n",
      "Epoch 2/3\n",
      "624/624 [==============================] - 6s 10ms/step - loss: 63.2798 - rmse: 7.3239 - val_loss: 59.1169 - val_rmse: 7.0806\n",
      "Epoch 3/3\n",
      "624/624 [==============================] - 6s 9ms/step - loss: 63.2478 - rmse: 7.3200 - val_loss: 59.1168 - val_rmse: 7.0797\n",
      "Epoch 1/5\n",
      "624/624 [==============================] - 20s 15ms/step - loss: 65.3693 - rmse: 7.3884 - val_loss: 59.2900 - val_rmse: 7.0715\n",
      "Epoch 2/5\n",
      "624/624 [==============================] - 7s 11ms/step - loss: 63.1898 - rmse: 7.3190 - val_loss: 59.2690 - val_rmse: 7.0686\n",
      "Epoch 3/5\n",
      "624/624 [==============================] - 7s 11ms/step - loss: 63.2399 - rmse: 7.3170 - val_loss: 59.2240 - val_rmse: 7.0738\n",
      "Epoch 4/5\n",
      "624/624 [==============================] - 7s 11ms/step - loss: 63.2844 - rmse: 7.3205 - val_loss: 59.2295 - val_rmse: 7.0684\n",
      "Epoch 5/5\n",
      "624/624 [==============================] - 6s 9ms/step - loss: 63.2280 - rmse: 7.3169 - val_loss: 59.2021 - val_rmse: 7.0751\n",
      "Epoch 1/7\n",
      "624/624 [==============================] - 20s 13ms/step - loss: 65.3858 - rmse: 7.3848 - val_loss: 59.1168 - val_rmse: 7.0774\n",
      "Epoch 2/7\n",
      "624/624 [==============================] - 6s 9ms/step - loss: 63.0837 - rmse: 7.3112 - val_loss: 59.1163 - val_rmse: 7.0774\n",
      "Epoch 3/7\n",
      "624/624 [==============================] - 6s 10ms/step - loss: 63.2113 - rmse: 7.3187 - val_loss: 59.1178 - val_rmse: 7.0729\n",
      "Epoch 4/7\n",
      "624/624 [==============================] - 7s 12ms/step - loss: 63.3088 - rmse: 7.3248 - val_loss: 59.1157 - val_rmse: 7.0753\n",
      "Epoch 5/7\n",
      "624/624 [==============================] - 6s 9ms/step - loss: 63.1481 - rmse: 7.3123 - val_loss: 59.1152 - val_rmse: 7.0762\n",
      "Epoch 6/7\n",
      "624/624 [==============================] - 6s 9ms/step - loss: 63.1329 - rmse: 7.3131 - val_loss: 59.1157 - val_rmse: 7.0746\n",
      "Epoch 7/7\n",
      "624/624 [==============================] - 7s 11ms/step - loss: 63.1195 - rmse: 7.3130 - val_loss: 59.1150 - val_rmse: 7.0758\n",
      "Epoch 1/10\n",
      "624/624 [==============================] - 19s 15ms/step - loss: 65.4297 - rmse: 7.3881 - val_loss: 59.1178 - val_rmse: 7.0830\n",
      "Epoch 2/10\n",
      "624/624 [==============================] - 8s 12ms/step - loss: 63.2405 - rmse: 7.3195 - val_loss: 59.1172 - val_rmse: 7.0816\n",
      "Epoch 3/10\n",
      "624/624 [==============================] - 8s 12ms/step - loss: 63.1772 - rmse: 7.3173 - val_loss: 59.1168 - val_rmse: 7.0798\n",
      "Epoch 4/10\n",
      "624/624 [==============================] - 9s 15ms/step - loss: 63.2114 - rmse: 7.3162 - val_loss: 59.1168 - val_rmse: 7.0776\n",
      "Epoch 5/10\n",
      "624/624 [==============================] - 10s 15ms/step - loss: 63.1813 - rmse: 7.3157 - val_loss: 59.1168 - val_rmse: 7.0775\n",
      "Epoch 6/10\n",
      "624/624 [==============================] - 9s 14ms/step - loss: 63.1181 - rmse: 7.3116 - val_loss: 59.1170 - val_rmse: 7.0810\n",
      "Epoch 7/10\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 63.2525 - rmse: 7.3205 - val_loss: 59.1169 - val_rmse: 7.0771\n",
      "Epoch 8/10\n",
      "624/624 [==============================] - 9s 14ms/step - loss: 63.1422 - rmse: 7.3122 - val_loss: 59.1168 - val_rmse: 7.0787\n",
      "Epoch 9/10\n",
      "624/624 [==============================] - 8s 13ms/step - loss: 63.0779 - rmse: 7.3099 - val_loss: 59.1169 - val_rmse: 7.0802\n",
      "Epoch 10/10\n",
      "624/624 [==============================] - 9s 14ms/step - loss: 63.1539 - rmse: 7.3138 - val_loss: 59.1168 - val_rmse: 7.0794\n",
      "Epoch 1/3\n",
      "312/312 [==============================] - 18s 19ms/step - loss: 66.5045 - rmse: 7.6502 - val_loss: 59.2796 - val_rmse: 7.2867\n",
      "Epoch 2/3\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.2496 - rmse: 7.5393 - val_loss: 59.2466 - val_rmse: 7.2876\n",
      "Epoch 3/3\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 63.1714 - rmse: 7.5340 - val_loss: 59.2289 - val_rmse: 7.2866\n",
      "Epoch 1/5\n",
      "312/312 [==============================] - 14s 16ms/step - loss: 66.9793 - rmse: 7.6716 - val_loss: 59.1170 - val_rmse: 7.2893\n",
      "Epoch 2/5\n",
      "312/312 [==============================] - 3s 10ms/step - loss: 63.2251 - rmse: 7.5366 - val_loss: 59.1170 - val_rmse: 7.2893\n",
      "Epoch 3/5\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 63.1059 - rmse: 7.5301 - val_loss: 59.1175 - val_rmse: 7.2902\n",
      "Epoch 4/5\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 63.1274 - rmse: 7.5325 - val_loss: 59.1176 - val_rmse: 7.2903\n",
      "Epoch 5/5\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 63.1395 - rmse: 7.5328 - val_loss: 59.1172 - val_rmse: 7.2896\n",
      "Epoch 1/7\n",
      "312/312 [==============================] - 19s 25ms/step - loss: 67.3994 - rmse: 7.6901 - val_loss: 59.1147 - val_rmse: 7.2868\n",
      "Epoch 2/7\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.3247 - rmse: 7.5425 - val_loss: 59.1143 - val_rmse: 7.2876\n",
      "Epoch 3/7\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.2088 - rmse: 7.5357 - val_loss: 59.1147 - val_rmse: 7.2890\n",
      "Epoch 4/7\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.1298 - rmse: 7.5325 - val_loss: 59.1154 - val_rmse: 7.2905\n",
      "Epoch 5/7\n",
      "312/312 [==============================] - 4s 11ms/step - loss: 63.1388 - rmse: 7.5308 - val_loss: 59.1150 - val_rmse: 7.2899\n",
      "Epoch 6/7\n",
      "312/312 [==============================] - 4s 11ms/step - loss: 63.2011 - rmse: 7.5353 - val_loss: 59.1160 - val_rmse: 7.2911\n",
      "Epoch 7/7\n",
      "312/312 [==============================] - 4s 13ms/step - loss: 63.0509 - rmse: 7.5267 - val_loss: 59.1178 - val_rmse: 7.2927\n",
      "Epoch 1/10\n",
      "312/312 [==============================] - 20s 23ms/step - loss: 67.2469 - rmse: 7.6769 - val_loss: 59.0928 - val_rmse: 7.2843\n",
      "Epoch 2/10\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.2989 - rmse: 7.5411 - val_loss: 59.0911 - val_rmse: 7.2854\n",
      "Epoch 3/10\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 63.1972 - rmse: 7.5379 - val_loss: 59.0940 - val_rmse: 7.2831\n",
      "Epoch 4/10\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 63.1712 - rmse: 7.5352 - val_loss: 59.0919 - val_rmse: 7.2876\n",
      "Epoch 5/10\n",
      "312/312 [==============================] - 4s 14ms/step - loss: 63.1314 - rmse: 7.5319 - val_loss: 59.0947 - val_rmse: 7.2867\n",
      "Epoch 6/10\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 63.1421 - rmse: 7.5312 - val_loss: 59.0970 - val_rmse: 7.2898\n",
      "Epoch 7/10\n",
      "312/312 [==============================] - 4s 14ms/step - loss: 63.0921 - rmse: 7.5269 - val_loss: 59.1009 - val_rmse: 7.2906\n",
      "Epoch 8/10\n",
      "312/312 [==============================] - 4s 14ms/step - loss: 63.1566 - rmse: 7.5326 - val_loss: 59.1013 - val_rmse: 7.2889\n",
      "Epoch 9/10\n",
      "312/312 [==============================] - 5s 15ms/step - loss: 63.1602 - rmse: 7.5332 - val_loss: 59.1027 - val_rmse: 7.2899\n",
      "Epoch 10/10\n",
      "312/312 [==============================] - 4s 12ms/step - loss: 63.1117 - rmse: 7.5308 - val_loss: 59.1024 - val_rmse: 7.2891\n",
      "Epoch 1/3\n",
      "156/156 [==============================] - 13s 24ms/step - loss: 68.6011 - rmse: 7.9665 - val_loss: 59.1446 - val_rmse: 7.4303\n",
      "Epoch 2/3\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.1068 - rmse: 7.6967 - val_loss: 59.1427 - val_rmse: 7.4305\n",
      "Epoch 3/3\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.1845 - rmse: 7.7031 - val_loss: 59.1443 - val_rmse: 7.4313\n",
      "Epoch 1/5\n",
      "156/156 [==============================] - 17s 28ms/step - loss: 69.6213 - rmse: 8.0208 - val_loss: 59.1607 - val_rmse: 7.4342\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 63.1634 - rmse: 7.7003 - val_loss: 59.1585 - val_rmse: 7.4339\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.1625 - rmse: 7.7000 - val_loss: 59.1562 - val_rmse: 7.4339\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 63.0969 - rmse: 7.6970 - val_loss: 59.1549 - val_rmse: 7.4345\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 63.2434 - rmse: 7.7050 - val_loss: 59.1513 - val_rmse: 7.4343\n",
      "Epoch 1/7\n",
      "156/156 [==============================] - 16s 27ms/step - loss: 68.4124 - rmse: 7.9596 - val_loss: 59.1617 - val_rmse: 7.4374\n",
      "Epoch 2/7\n",
      "156/156 [==============================] - 2s 11ms/step - loss: 63.2537 - rmse: 7.7058 - val_loss: 59.1606 - val_rmse: 7.4371\n",
      "Epoch 3/7\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 63.1162 - rmse: 7.6970 - val_loss: 59.1599 - val_rmse: 7.4367\n",
      "Epoch 4/7\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 63.1414 - rmse: 7.7008 - val_loss: 59.1578 - val_rmse: 7.4366\n",
      "Epoch 5/7\n",
      "156/156 [==============================] - 2s 10ms/step - loss: 63.0512 - rmse: 7.6928 - val_loss: 59.1618 - val_rmse: 7.4375\n",
      "Epoch 6/7\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 63.2311 - rmse: 7.7053 - val_loss: 59.1574 - val_rmse: 7.4365\n",
      "Epoch 7/7\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 63.1496 - rmse: 7.6998 - val_loss: 59.1603 - val_rmse: 7.4380\n",
      "Epoch 1/10\n",
      "156/156 [==============================] - 19s 29ms/step - loss: 70.8558 - rmse: 8.0857 - val_loss: 59.1394 - val_rmse: 7.4369\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.1895 - rmse: 7.7030 - val_loss: 59.1405 - val_rmse: 7.4372\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 63.1711 - rmse: 7.6988 - val_loss: 59.1396 - val_rmse: 7.4370\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 3s 16ms/step - loss: 63.0565 - rmse: 7.6941 - val_loss: 59.1436 - val_rmse: 7.4379\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.0203 - rmse: 7.6912 - val_loss: 59.1454 - val_rmse: 7.4383\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 2s 11ms/step - loss: 63.2765 - rmse: 7.7073 - val_loss: 59.1379 - val_rmse: 7.4365\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 2s 11ms/step - loss: 63.1726 - rmse: 7.7009 - val_loss: 59.1375 - val_rmse: 7.4364\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 2s 12ms/step - loss: 63.1744 - rmse: 7.7013 - val_loss: 59.1407 - val_rmse: 7.4372\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 2s 10ms/step - loss: 63.1619 - rmse: 7.7008 - val_loss: 59.1380 - val_rmse: 7.4366\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 2s 10ms/step - loss: 63.1013 - rmse: 7.6967 - val_loss: 59.1389 - val_rmse: 7.4368\n",
      "Best score: [59.09693908691406, 7.277767181396484], with parameters: {'batch_size': 8, 'epochs': 7}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [8, 16],\n",
    "    'epochs': [3, 5]\n",
    "}\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def create_compile_model(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(80, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "best_score = None\n",
    "best_params = None\n",
    "\n",
    "for batch_size in param_grid['batch_size']:\n",
    "    for epochs in param_grid['epochs']:\n",
    "\n",
    "            model = create_compile_model('adam')\n",
    "\n",
    "            model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1, shuffle=False)\n",
    "\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "            if best_score is None or score < best_score:\n",
    "                best_score = score\n",
    "                best_params = {'batch_size': batch_size, 'epochs': epochs}\n",
    "\n",
    "print(f'Best score: {best_score}, with parameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
